{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfdccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shlex\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for python3: read in python2 pickled files\n",
    "import _pickle as cPickle\n",
    "\n",
    "import gzip\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import cv2\n",
    "from parmap import parmap\n",
    "\n",
    "def parseArgs(parser):\n",
    "    parser.add_argument('--labels_test', \n",
    "                        help='contains test images/descriptors to load + labels')\n",
    "    parser.add_argument('--labels_train', \n",
    "                        help='contains training images/descriptors to load + labels')\n",
    "    parser.add_argument('-s', '--suffix',\n",
    "                        default='_SIFT_patch_pr.pkl.gz',\n",
    "                        help='only chose those images with a specific suffix')\n",
    "    parser.add_argument('--in_test',\n",
    "                        help='the input folder of the test images / features')\n",
    "    parser.add_argument('--in_train',\n",
    "                        help='the input folder of the training images / features')\n",
    "    parser.add_argument('--overwrite', action='store_true',\n",
    "                        help='do not load pre-computed encodings')\n",
    "    parser.add_argument('--powernorm', action='store_true',\n",
    "                        help='use powernorm')\n",
    "    parser.add_argument('--gmp', action='store_true',\n",
    "                        help='use generalized max pooling')\n",
    "    parser.add_argument('--gamma', default=1, type=float,\n",
    "                        help='regularization parameter of GMP')\n",
    "    parser.add_argument('--C', default=1000, type=float, \n",
    "                        help='C parameter of the SVM')\n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "935ab306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(folder, pattern, labelfile):\n",
    "    \"\"\" \n",
    "    returns files and associated labels by reading the labelfile \n",
    "    parameters:\n",
    "        folder: inputfolder\n",
    "        pattern: new suffix\n",
    "        labelfiles: contains a list of filename and labels\n",
    "    return: absolute filenames + labels \n",
    "    \"\"\"\n",
    "    # read labelfile\n",
    "    with open(labelfile, 'r') as f:\n",
    "        all_lines = f.readlines()\n",
    "    \n",
    "    # get filenames from labelfile\n",
    "    all_files = []\n",
    "    labels = []\n",
    "    check = True\n",
    "    for line in all_lines:\n",
    "        # using shlex we also allow spaces in filenames when escaped w. \"\"\n",
    "        splits = shlex.split(line)\n",
    "        file_name = splits[0]\n",
    "        class_id = splits[1]\n",
    "\n",
    "        # strip all known endings, note: os.path.splitext() doesnt work for\n",
    "        # '.' in the filenames, so let's do it this way...\n",
    "        for p in ['.pkl.gz', '.txt', '.png', '.jpg', '.tif', '.ocvmb','.csv']:\n",
    "            if file_name.endswith(p):\n",
    "                file_name = file_name.replace(p,'')\n",
    "\n",
    "        # get now new file name\n",
    "        true_file_name = os.path.join(folder, file_name + pattern)\n",
    "        all_files.append(true_file_name)\n",
    "        labels.append(class_id)\n",
    "\n",
    "    return all_files, labels\n",
    "\n",
    "def loadRandomDescriptors(files, max_descriptors):\n",
    "    \"\"\" \n",
    "    load roughly `max_descriptors` random descriptors\n",
    "    parameters:\n",
    "        files: list of filenames containing local features of dimension D\n",
    "        max_descriptors: maximum number of descriptors (Q)\n",
    "    returns: QxD matrix of descriptors\n",
    "    \"\"\"\n",
    "    # let's just take 100 files to speed-up the process\n",
    "    max_files = 100\n",
    "    indices = np.random.permutation(max_files)\n",
    "    files = np.array(files)[indices]\n",
    "   \n",
    "    # rough number of descriptors per file that we have to load\n",
    "    max_descs_per_file = int(max_descriptors / len(files))\n",
    "\n",
    "    descriptors = []\n",
    "    for i in tqdm(range(len(files))):\n",
    "        with gzip.open(files[i], 'rb') as ff:\n",
    "            # for python2\n",
    "            # desc = cPickle.load(ff)\n",
    "            # for python3\n",
    "            desc = cPickle.load(ff, encoding='latin1')\n",
    "            \n",
    "        # get some random ones\n",
    "        indices = np.random.choice(len(desc),\n",
    "                                   min(len(desc),\n",
    "                                       int(max_descs_per_file)),\n",
    "                                   replace=False)\n",
    "        desc = desc[ indices ]\n",
    "        descriptors.append(desc)\n",
    "    \n",
    "    descriptors = np.concatenate(descriptors, axis=0)\n",
    "    return descriptors\n",
    "\n",
    "def dictionary(descriptors, n_clusters):\n",
    "    \"\"\" \n",
    "    return cluster centers for the descriptors \n",
    "    parameters:\n",
    "        descriptors: NxD matrix of local descriptors\n",
    "        n_clusters: number of clusters = K\n",
    "    returns: KxD matrix of K clusters\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    dummy = np.array([42])\n",
    "    return dummy\n",
    "def assignments(descriptors, clusters):\n",
    "    \"\"\" \n",
    "    compute assignment matrix\n",
    "    parameters:\n",
    "        descriptors: TxD descriptor matrix\n",
    "        clusters: KxD cluster matrix\n",
    "    returns: TxK assignment matrix\n",
    "    \"\"\"\n",
    "    # compute nearest neighbors\n",
    "    # TODO\n",
    "\n",
    "    # create hard assignment\n",
    "    assignment = np.zeros( (len(descriptors), len(clusters)) )\n",
    "    # TODO\n",
    "\n",
    "    return assignment\n",
    "\n",
    "def vlad(files, mus, powernorm, gmp=False, gamma=1000):\n",
    "    \"\"\"\n",
    "    compute VLAD encoding for each files\n",
    "    parameters: \n",
    "        files: list of N files containing each T local descriptors of dimension\n",
    "        D\n",
    "        mus: KxD matrix of cluster centers\n",
    "        gmp: if set to True use generalized max pooling instead of sum pooling\n",
    "    returns: NxK*D matrix of encodings\n",
    "    \"\"\"\n",
    "    K = mus.shape[0]\n",
    "    encodings = []\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        with gzip.open(f, 'rb') as ff:\n",
    "            desc = cPickle.load(ff, encoding='latin1')\n",
    "        a = assignments(desc, mus)\n",
    "        \n",
    "        T,D = desc.shape\n",
    "        f_enc = np.zeros( (D*K), dtype=np.float32)\n",
    "        for k in range(mus.shape[0]):\n",
    "            # it's faster to select only those descriptors that have\n",
    "            # this cluster as nearest neighbor and then compute the \n",
    "            # difference to the cluster center than computing the differences\n",
    "            # first and then select\n",
    "            pass\n",
    "\n",
    "   \n",
    "        # c) power normalization\n",
    "        if powernorm:\n",
    "            # TODO\n",
    "            pass\n",
    "        \n",
    "\n",
    "        # l2 normalization\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    return encodings\n",
    "\n",
    "def esvm(encs_test, encs_train, C=1000):\n",
    "    \"\"\" \n",
    "    compute a new embedding using Exemplar Classification\n",
    "    compute for each encs_test encoding an E-SVM using the\n",
    "    encs_train as negatives   \n",
    "    parameters: \n",
    "        encs_test: NxD matrix\n",
    "        encs_train: MxD matrix\n",
    "\n",
    "    returns: new encs_test matrix (NxD)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # set up labels\n",
    "    # TODO\n",
    "\n",
    "    def loop(i):\n",
    "        # compute SVM \n",
    "        # and make feature transformation\n",
    "        # TODO\n",
    "        return x\n",
    "\n",
    "    # let's do that in parallel: \n",
    "    # if that doesn't work for you, just exchange 'parmap' with 'map'\n",
    "    # Even better: use DASK arrays instead, then everything should be\n",
    "    # parallelized\n",
    "    new_encs = list(parmap( loop, tqdm(range(len(encs_test)))))\n",
    "    new_encs = np.concatenate(new_encs, axis=0)\n",
    "    # return new encodings\n",
    "    return new_encs\n",
    "\n",
    "\n",
    "def distances(encs):\n",
    "    \"\"\" \n",
    "    compute pairwise distances \n",
    "\n",
    "    parameters:\n",
    "        encs:  TxK*D encoding matrix\n",
    "    returns: TxT distance matrix\n",
    "    \"\"\"\n",
    "    # compute cosine distance = 1 - dot product between l2-normalized\n",
    "    # encodings\n",
    "    # TODO\n",
    "    # mask out distance with itself\n",
    "    np.fill_diagonal(dists, np.finfo(dists.dtype).max)\n",
    "    return dists\n",
    "\n",
    "def evaluate(encs, labels):\n",
    "    \"\"\"\n",
    "    evaluate encodings assuming using associated labels\n",
    "    parameters:\n",
    "        encs: TxK*D encoding matrix\n",
    "        labels: array/list of T labels\n",
    "    \"\"\"\n",
    "    dist_matrix = distances(encs)\n",
    "    # sort each row of the distance matrix\n",
    "    indices = dist_matrix.argsort()\n",
    "\n",
    "    n_encs = len(encs)\n",
    "\n",
    "    mAP = []\n",
    "    correct = 0\n",
    "    for r in range(n_encs):\n",
    "        precisions = []\n",
    "        rel = 0\n",
    "        for k in range(n_encs-1):\n",
    "            if labels[ indices[r,k] ] == labels[ r ]:\n",
    "                rel += 1\n",
    "                precisions.append( rel / float(k+1) )\n",
    "                if k == 0:\n",
    "                    correct += 1\n",
    "        avg_precision = np.mean(precisions)\n",
    "        mAP.append(avg_precision)\n",
    "    mAP = np.mean(mAP)\n",
    "\n",
    "    print('Top-1 accuracy: {} - mAP: {}'.format(float(correct) / n_encs, mAP))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a592e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: retrieval [-h] [--labels_test LABELS_TEST]\n",
      "                 [--labels_train LABELS_TRAIN] [-s SUFFIX] [--in_test IN_TEST]\n",
      "                 [--in_train IN_TRAIN] [--overwrite] [--powernorm] [--gmp]\n",
      "                 [--gamma GAMMA] [--C C]\n",
      "retrieval: error: unrecognized arguments: -f C:\\Users\\vicky\\AppData\\Roaming\\jupyter\\runtime\\kernel-579e8f75-4836-4c49-b3d5-544f5b035a12.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('retrieval')\n",
    "    parser = parseArgs(parser)\n",
    "    args = parser.parse_args()\n",
    "    np.random.seed(42) # fix random seed\n",
    "   \n",
    "    # a) dictionary\n",
    "    files_train, labels_train = getFiles(args.in_train, args.suffix,\n",
    "                                         args.labels_train)\n",
    "    print('#train: {}'.format(len(files_train)))\n",
    "    if not os.path.exists('mus.pkl.gz'):\n",
    "        # TODO\n",
    "        print('> loaded {} descriptors:'.format(len(descriptors)))\n",
    "\n",
    "        # cluster centers\n",
    "        print('> compute dictionary')\n",
    "        # TODO\n",
    "        with gzip.open('mus.pkl.gz', 'wb') as fOut:\n",
    "            cPickle.dump(mus, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open('mus.pkl.gz', 'rb') as f:\n",
    "            mus = cPickle.load(f)\n",
    "\n",
    "  \n",
    "    # b) VLAD encoding\n",
    "    print('> compute VLAD for test')\n",
    "    files_test, labels_test = getFiles(args.in_test, args.suffix,\n",
    "                                       args.labels_test)\n",
    "    print('#test: {}'.format(len(files_test)))\n",
    "    fname = 'enc_test_gmp{}.pkl.gz'.format(gamma) if args.gmp else 'enc_test.pkl.gz'\n",
    "    if not os.path.exists(fname) or args.overwrite:\n",
    "        # TODO\n",
    "        with gzip.open(fname, 'wb') as fOut:\n",
    "            cPickle.dump(enc_test, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            enc_test = cPickle.load(f)\n",
    "   \n",
    "    # cross-evaluate test encodings\n",
    "    print('> evaluate')\n",
    "    evaluate(enc_test, labels_test)\n",
    "\n",
    "    # d) compute exemplar svms\n",
    "    print('> compute VLAD for train (for E-SVM)')\n",
    "    fname = 'enc_train_gmp{}.pkl.gz'.format(gamma) if args.gmp else 'enc_train.pkl.gz'\n",
    "    if not os.path.exists(fname) or args.overwrite:\n",
    "        # TODO\n",
    "        with gzip.open(fname, 'wb') as fOut:\n",
    "            cPickle.dump(enc_train, fOut, -1)\n",
    "    else:\n",
    "        with gzip.open(fname, 'rb') as f:\n",
    "            enc_train = cPickle.load(f)\n",
    "\n",
    "    print('> esvm computation')\n",
    "    # TODO\n",
    "\n",
    "    # eval\n",
    "    evaluate(enc_test, labels_test)\n",
    "    print('> evaluate')\n",
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd829e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
